{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Data Preprocessing"
      ],
      "metadata": {
        "id": "4TVCN9CctRtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image as pilImg\n",
        "import os \n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import optimizers\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from prettytable import PrettyTable"
      ],
      "metadata": {
        "id": "QlJt4EuitXIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.dropbox.com/s/y0jjlw4gvju9llf/parser.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cI2dEop0R3y",
        "outputId": "e930ea00-a766-42cc-e867-e3837e91aa41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 13:17:02--  https://www.dropbox.com/s/y0jjlw4gvju9llf/parser.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/y0jjlw4gvju9llf/parser.txt [following]\n",
            "--2023-01-10 13:17:02--  https://www.dropbox.com/s/raw/y0jjlw4gvju9llf/parser.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com/cd/0/inline/B0Ttjo5wiAR1vLR0YJgS7TnFKXt5Cr9NJR1qIFfnaPnuurYOD12mKTKbleZF3j-iIbKzQjK5c9x7JxIAjgJSLglhC9oY-xV5UfSG2U9upw0aWF1SxRNYebSvNFNLkJexaqBuhU1pwwVzNyxOkOf34p3Au7KvnoBYSBE_32gwX_k9AQ/file# [following]\n",
            "--2023-01-10 13:17:02--  https://uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com/cd/0/inline/B0Ttjo5wiAR1vLR0YJgS7TnFKXt5Cr9NJR1qIFfnaPnuurYOD12mKTKbleZF3j-iIbKzQjK5c9x7JxIAjgJSLglhC9oY-xV5UfSG2U9upw0aWF1SxRNYebSvNFNLkJexaqBuhU1pwwVzNyxOkOf34p3Au7KvnoBYSBE_32gwX_k9AQ/file\n",
            "Resolving uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com (uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com (uc02dc10a5e0c58bd7bbe6272367.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5297020 (5.1M) [text/plain]\n",
            "Saving to: ‘parser.txt?dl=0’\n",
            "\n",
            "parser.txt?dl=0     100%[===================>]   5.05M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-01-10 13:17:03 (125 MB/s) - ‘parser.txt?dl=0’ saved [5297020/5297020]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.dropbox.com/s/0qtt1j8zfbxowet/words2.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M_qfTEb0Xzk",
        "outputId": "a9369742-5d45-492a-82f2-47e75d13bc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 13:17:57--  https://www.dropbox.com/s/0qtt1j8zfbxowet/words2.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/0qtt1j8zfbxowet/words2.zip [following]\n",
            "--2023-01-10 13:17:57--  https://www.dropbox.com/s/raw/0qtt1j8zfbxowet/words2.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com/cd/0/inline/B0QbPKp8nJf92ZqKrf8cBawi3xE2NNWKri3LU6NRy5qi9lk-E9Yzhrhlim9_ySh5Dd305z0ZHufLKPF4dKFg5ug9KdmvwZ2gQGQwsjTS5k2ArMb6V63Rg8jt0oJI81THJhWRLhru_SplNKdugyrcjlczr69amSy_7ihb7SKTbBZgpQ/file# [following]\n",
            "--2023-01-10 13:17:57--  https://uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com/cd/0/inline/B0QbPKp8nJf92ZqKrf8cBawi3xE2NNWKri3LU6NRy5qi9lk-E9Yzhrhlim9_ySh5Dd305z0ZHufLKPF4dKFg5ug9KdmvwZ2gQGQwsjTS5k2ArMb6V63Rg8jt0oJI81THJhWRLhru_SplNKdugyrcjlczr69amSy_7ihb7SKTbBZgpQ/file\n",
            "Resolving uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com (uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com (uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B0Qhdg02vkcOQh57xxzX3pMiWzHRLQNUOOdfSy6-n-dP2PO9CfuPveI6kJgQuxXjxx3tJYKHsORRR3DSAKGCIugTToy9LTEMohfydb6py_t_pgHUAKo-Vqk9flLkmTOH4VWTEEUn8D-Os96jdiYBqMjbqzwuW_Vq_Tp13M2KdTf3zy3zlE0OYEzPhGsjHR4m7EW1SYB-rOSrAz3ZptVM8PLbwPGkmVipefFN8Y9Grups5LXXWsJfEdS9EUnPiyGyFIyZ2-q7WoppHZ2rOi0Trc8SaKdX-2kcRCozAZyBxID5dnv_BZRTiW3uUedTDDLcTELPuBmuA62soC6A_NcQVx_rPS0OQ-Wp3ocyaiaLyxuxZaMqUGC_5SzxEkspOZJJVuqNG-ew8jxb_IixIAzfxa2RyZQGoijT4VFzR7XHjMwEbg/file [following]\n",
            "--2023-01-10 13:17:58--  https://uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com/cd/0/inline2/B0Qhdg02vkcOQh57xxzX3pMiWzHRLQNUOOdfSy6-n-dP2PO9CfuPveI6kJgQuxXjxx3tJYKHsORRR3DSAKGCIugTToy9LTEMohfydb6py_t_pgHUAKo-Vqk9flLkmTOH4VWTEEUn8D-Os96jdiYBqMjbqzwuW_Vq_Tp13M2KdTf3zy3zlE0OYEzPhGsjHR4m7EW1SYB-rOSrAz3ZptVM8PLbwPGkmVipefFN8Y9Grups5LXXWsJfEdS9EUnPiyGyFIyZ2-q7WoppHZ2rOi0Trc8SaKdX-2kcRCozAZyBxID5dnv_BZRTiW3uUedTDDLcTELPuBmuA62soC6A_NcQVx_rPS0OQ-Wp3ocyaiaLyxuxZaMqUGC_5SzxEkspOZJJVuqNG-ew8jxb_IixIAzfxa2RyZQGoijT4VFzR7XHjMwEbg/file\n",
            "Reusing existing connection to uc572f72c2b36054b33d5dbcf3e4.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 54336221 (52M) [application/zip]\n",
            "Saving to: ‘words2.zip’\n",
            "\n",
            "words2.zip          100%[===================>]  51.82M   200MB/s    in 0.3s    \n",
            "\n",
            "2023-01-10 13:17:58 (200 MB/s) - ‘words2.zip’ saved [54336221/54336221]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/words2.zip'"
      ],
      "metadata": {
        "id": "qAGKvfGztpZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/words2/'\n",
        "#/content/words2/a01/a01-000u/a01-000u-00-00.png\n",
        "df2 = pd.read_csv(\"/content/parser.txt\", sep=\" \", header=None, \n",
        "                 names=[\"ImageId\", '0','1','2','3','4','5','6','Labels'])\n",
        "path =df2['ImageId'].str.split(\"-\", n = 3, expand = True)\n",
        "df2['ImageName']=directory_path+path[0]+'/'+path[0]+'-'+path[1]+'/'+df2['ImageId']+'.png'\n",
        "df2['folder']=path[0]\n",
        "df2 =df2[df2['folder'] =='a01']\n",
        "dataFrame_ = df2[['ImageName','Labels']]"
      ],
      "metadata": {
        "id": "CtNXHPWhuT8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df, val_data = train_test_split(dataFrame_, test_size=0.2)"
      ],
      "metadata": {
        "id": "lv5HgkupuYWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(df, test_size=0.2)"
      ],
      "metadata": {
        "id": "QakYzZcQuaJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('Train_data.csv')\n",
        "val_data.to_csv('Validation_data.csv')\n",
        "test_data.to_csv('Test_data.csv')"
      ],
      "metadata": {
        "id": "sV1DKIkGucBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('Train_data.csv')\n",
        "train_data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "train_files=train_data['ImageName'].values\n",
        "train_data.to_csv('Train_Final.csv')"
      ],
      "metadata": {
        "id": "PDMy0zUBvugP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data=pd.read_csv('Validation_data.csv')\n",
        "val_data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "val_files=val_data['ImageName'].values\n",
        "val_data.to_csv('Validation_Final.csv')"
      ],
      "metadata": {
        "id": "SpQ1Yz1tv_F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=pd.read_csv('Test_data.csv')\n",
        "test_data.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "test_files=test_data['ImageName'].values\n",
        "test_data.to_csv('Test_Final.csv')"
      ],
      "metadata": {
        "id": "qJgsgVVdwJse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('Train_Final.csv')\n",
        "val_data=pd.read_csv('Validation_Final.csv')\n",
        "test_data=pd.read_csv('Test_Final.csv')"
      ],
      "metadata": {
        "id": "yr9dVVlvwUcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzllN36NN9zn"
      },
      "source": [
        "## 8. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJLnsmvmN9zn"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import random\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAXacZstN9zo"
      },
      "outputs": [],
      "source": [
        "#Letters present in the Label Text\n",
        "letters= '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1kjuPPHN9zo"
      },
      "outputs": [],
      "source": [
        "img_h=32\n",
        "img_w=170\n",
        "img_c=1\n",
        "num_classes=len(letters)+1\n",
        "batch_size=64\n",
        "max_length=15 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCXTzHnHN9zo"
      },
      "outputs": [],
      "source": [
        "def encode_words_labels(word):\n",
        "    label_lst=[]\n",
        "    for char in word:\n",
        "        label_lst.append(letters.find(char)) \n",
        "    return label_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Femy_Pp1N9zo"
      },
      "outputs": [],
      "source": [
        "def words_from_labels(labels):\n",
        "    txt=[]\n",
        "    for ele in labels:\n",
        "        if ele == len(letters): \n",
        "            txt.append(\"\")\n",
        "        else:\n",
        "            txt.append(letters[ele])\n",
        "    return \"\".join(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkTkKddeN9zo"
      },
      "outputs": [],
      "source": [
        "def ctc_loss_function(args):\n",
        "\n",
        "    y_pred, y_true, input_length, label_length = args \n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exWlAfTGN9zo"
      },
      "source": [
        "## 9. Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RKqoq3pN9zp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, img_dirpath, img_w, img_h,\n",
        "                 batch_size,n,output_labels,max_text_len=15):\n",
        "        self.img_h = img_h                    \n",
        "        self.img_w = img_w                   \n",
        "        self.batch_size = batch_size         \n",
        "        self.max_text_len = max_text_len    \n",
        "        self.n=n\n",
        "        self.img_dir = img_dirpath[:self.n]   \n",
        "        self.indexes = list(range(self.n))   \n",
        "        self.cur_index = 0                   \n",
        "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
        "        self.texts =  output_labels[:self.n]                 \n",
        "\n",
        "   \n",
        "    def build_data(self):\n",
        "        print(self.n, \" Image Loading start...\")\n",
        "        for i, img_file in enumerate(self.img_dir):\n",
        "            img = cv2.imread(img_file)\n",
        "            img = img[:,:,1]                              \n",
        "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
        "            img = img /255\n",
        "            self.imgs[i, :, :]= img\n",
        "            if i%10000==0:\n",
        "                print(\"Loaded Images: \",i)\n",
        "           \n",
        "        print(\"Number of Texts matches with Total Number of Images :\",len(self.texts) == self.n)\n",
        "        print(self.n, \" Image Loading finish...\")\n",
        "\n",
        "\n",
        "    def next_data(self): \n",
        "        \"\"\"\n",
        "        Returns image and text data pointed by the current index\n",
        "        \"\"\"\n",
        "        self.cur_index += 1\n",
        "        if self.cur_index >= self.n:\n",
        "            self.cur_index = 0\n",
        "            random.shuffle(self.indexes)\n",
        "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"\n",
        "        Creates a batch of images images and text data equal to the batch_size,\n",
        "        computes the parameters needed for CTC and returns the inputs to the Model\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])  \n",
        "            Y_data = np.ones([self.batch_size, self.max_text_len])* -1       \n",
        "            input_length = np.ones((self.batch_size, 1)) * 40\n",
        "            label_length = np.zeros((self.batch_size, 1))                   \n",
        "            source_str=[]                                                  \n",
        "            for i in range(self.batch_size):\n",
        "                img, text = self.next_data() \n",
        "                img=img.T\n",
        "                img = np.expand_dims(img, -1) \n",
        "                X_data[i] = img\n",
        "                label=encode_words_labels(text) \n",
        "                lbl_len=len(label)\n",
        "                Y_data[i,0:lbl_len] = label \n",
        "                label_length[i] = len(label)\n",
        "                source_str.append(text) #\n",
        "            inputs = {\n",
        "                'img_input': X_data,  \n",
        "                'ground_truth_labels': Y_data,  \n",
        "                'input_length': input_length,  \n",
        "                'label_length': label_length,\n",
        "                'source_str': source_str \n",
        "            }\n",
        "            outputs = {'ctc': np.zeros([self.batch_size])}  \n",
        "            yield (inputs, outputs) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UveqF52bN9zq"
      },
      "source": [
        "## 12. Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm0n93-8N9zq"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Conv2D, MaxPool2D, Dense,MaxPooling2D\n",
        "from keras.layers import AveragePooling2D, Flatten, Activation, Bidirectional\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "from keras.layers import Concatenate, Add, Multiply, Lambda\n",
        "from keras.layers import UpSampling2D, Reshape\n",
        "from tensorflow.keras.layers import add,concatenate\n",
        "from keras.layers import Reshape\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM,GRU\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftA5mKoN9zq"
      },
      "source": [
        "## 12.1. Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP4oP7I8N9zq"
      },
      "source": [
        "**Model with Bi-Directional LSTM units and Adam Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V665TJANN9zq"
      },
      "outputs": [],
      "source": [
        "def Image_text_recogniser_model_1(stage,drop_out_rate=0.35):\n",
        "    \"\"\"\n",
        "    Builds the model by taking in the stage variable which specifes the stage,\n",
        "    if the stage is training: model takes inputs required for computing ctc_batch_cost function\n",
        "    else : model takes input as images which is used for prediction\n",
        "    \"\"\"\n",
        "    \n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        input_shape = (1, img_w, img_h)\n",
        "    else:\n",
        "        input_shape = (img_w, img_h, 1)\n",
        "       \n",
        "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
        "\n",
        "    # Convolution layer \n",
        "    model = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(model_input) \n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = MaxPooling2D(pool_size=(2, 2), name='max1')(model) \n",
        "\n",
        "    model = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(model) \n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = MaxPooling2D(pool_size=(2, 2), name='max2')(model) \n",
        "\n",
        "    model = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(model) \n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(model)\n",
        "    model=Dropout(drop_out_rate)(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = MaxPooling2D(pool_size=(1, 2), name='max3')(model)  \n",
        "\n",
        "    model = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(model) \n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = Conv2D(512, (3, 3), padding='same', name='conv6')(model)\n",
        "    model=Dropout(drop_out_rate)(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)\n",
        "    model = MaxPooling2D(pool_size=(1, 2), name='max4')(model) \n",
        "\n",
        "    model = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(model)\n",
        "    model=Dropout(0.25)(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = Activation('relu')(model)    \n",
        "\n",
        "    # CNN to RNN\n",
        "    model = Reshape(target_shape=((42, 1024)), name='reshape')(model)  \n",
        "    model = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(model)  \n",
        "\n",
        "    # RNN layer\n",
        "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(model)\n",
        "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
        "\n",
        "    # transforms RNN output to character activations:\n",
        "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model) \n",
        "    y_pred = Activation('softmax', name='softmax')(model)\n",
        "\n",
        "    \n",
        "    labels = Input(name='ground_truth_labels', shape=[max_length], dtype='float32') \n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
        "\n",
        "    #CTC loss function\n",
        "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
        "\n",
        "    if stage=='train':\n",
        "        return model_input,y_pred,Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
        "    else:\n",
        "        return Model(inputs=[model_input], outputs=y_pred)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbts3FHSN9zr"
      },
      "outputs": [],
      "source": [
        "model_input,y_pred,img_text_recog=Image_text_recogniser_model_1('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnwc6WmlN9zr"
      },
      "outputs": [],
      "source": [
        "test_func = K.function([model_input], [y_pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwmovdrYN9zr",
        "outputId": "f9a82c2f-07aa-4548-dbad-07aa385b340d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " img_input (InputLayer)         [(None, 170, 32, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)                 (None, 170, 32, 64)  640         ['img_input[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 170, 32, 64)  256        ['conv1[0][0]']                  \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 170, 32, 64)  0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " max1 (MaxPooling2D)            (None, 85, 16, 64)   0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2 (Conv2D)                 (None, 85, 16, 128)  73856       ['max1[0][0]']                   \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 85, 16, 128)  512        ['conv2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 85, 16, 128)  0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " max2 (MaxPooling2D)            (None, 42, 8, 128)   0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv3 (Conv2D)                 (None, 42, 8, 256)   295168      ['max2[0][0]']                   \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 42, 8, 256)  1024        ['conv3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 42, 8, 256)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv4 (Conv2D)                 (None, 42, 8, 256)   590080      ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 42, 8, 256)   0           ['conv4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 42, 8, 256)  1024        ['dropout[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 42, 8, 256)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max3 (MaxPooling2D)            (None, 42, 4, 256)   0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv5 (Conv2D)                 (None, 42, 4, 512)   1180160     ['max3[0][0]']                   \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 42, 4, 512)  2048        ['conv5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 42, 4, 512)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv6 (Conv2D)                 (None, 42, 4, 512)   2359808     ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 42, 4, 512)   0           ['conv6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 42, 4, 512)  2048        ['dropout_1[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 42, 4, 512)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " max4 (MaxPooling2D)            (None, 42, 2, 512)   0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " con7 (Conv2D)                  (None, 42, 2, 512)   1049088     ['max4[0][0]']                   \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 42, 2, 512)   0           ['con7[0][0]']                   \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 42, 2, 512)  2048        ['dropout_2[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 42, 2, 512)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 42, 1024)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dense1 (Dense)                 (None, 42, 64)       65600       ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 42, 256)      657408      ['dense1[0][0]']                 \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 42, 512)     1050624     ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense2 (Dense)                 (None, 42, 37)       18981       ['bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " softmax (Activation)           (None, 42, 37)       0           ['dense2[0][0]']                 \n",
            "                                                                                                  \n",
            " ground_truth_labels (InputLaye  [(None, 15)]        0           []                               \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " input_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " label_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " ctc (Lambda)                   (None, 1)            0           ['softmax[0][0]',                \n",
            "                                                                  'ground_truth_labels[0][0]',    \n",
            "                                                                  'input_length[0][0]',           \n",
            "                                                                  'label_length[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,350,373\n",
            "Trainable params: 7,345,893\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "img_text_recog.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4mMAiMkN9zr"
      },
      "outputs": [],
      "source": [
        "def decode_batch(test_func, word_batch):\n",
        "    \"\"\"\n",
        "    Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n",
        "    \"\"\"\n",
        "    out = test_func([word_batch])[0] \n",
        "    ret = []\n",
        "    for j in range(out.shape[0]):\n",
        "        out_best = list(np.argmax(out[j, 2:], 1))\n",
        "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
        "        outstr = words_from_labels(out_best)\n",
        "        ret.append(outstr)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUPihpzKN9zr"
      },
      "outputs": [],
      "source": [
        "def accuracies(actual_labels,predicted_labels,is_train):\n",
        "    \"\"\"\n",
        "    Takes a List of Actual Outputs, predicted Outputs and returns their accuracy and letter accuracy across\n",
        "    all the labels in the list\n",
        "    \"\"\"\n",
        "    accuracy=0\n",
        "    letter_acc=0\n",
        "    letter_cnt=0\n",
        "    count=0\n",
        "    for i in range(len(actual_labels)):\n",
        "        predicted_output=predicted_labels[i]\n",
        "        actual_output=actual_labels[i]\n",
        "        count+=1\n",
        "        for j in range(min(len(predicted_output),len(actual_output))):\n",
        "            if predicted_output[j]==actual_output[j]:\n",
        "                letter_acc+=1\n",
        "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
        "        if actual_output==predicted_output:\n",
        "            accuracy+=1\n",
        "    final_accuracy=np.round((accuracy/len(actual_labels))*100,2)\n",
        "    final_letter_acc=np.round((letter_acc/letter_cnt)*100,2)\n",
        "    return final_accuracy,final_letter_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofggd53dN9zr"
      },
      "source": [
        "**CallBacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy2pIw6lN9zs"
      },
      "outputs": [],
      "source": [
        "class VizCallback(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    The Custom Callback created for printing the Accuracy and Letter Accuracy Metrics at the End of Each Epoch\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, test_func, text_img_gen,is_train,acc_compute_batches):\n",
        "        self.test_func = test_func\n",
        "        self.text_img_gen = text_img_gen\n",
        "        self.is_train=is_train                \n",
        "        self.acc_batches=acc_compute_batches \n",
        "\n",
        "    def show_accuracy_metrics(self,num_batches):\n",
        "        \"\"\"\n",
        "        Calculates the accuracy and letter accuracy for each batch of inputs, \n",
        "        and prints the avarage accuracy and letter accuracy across all the batches\n",
        "        \"\"\"\n",
        "        accuracy=0\n",
        "        letter_accuracy=0\n",
        "        batches_cnt=num_batches\n",
        "        while batches_cnt>0:\n",
        "            word_batch = next(self.text_img_gen)[0]   #Gets the next batch from the Data generator\n",
        "            decoded_res = decode_batch(self.test_func,word_batch['img_input'])\n",
        "            actual_res=word_batch['source_str']\n",
        "            acc,let_acc=accuracies(actual_res,decoded_res,self.is_train)\n",
        "            accuracy+=acc\n",
        "            letter_accuracy+=let_acc\n",
        "            batches_cnt-=1\n",
        "        accuracy=accuracy/num_batches\n",
        "        letter_accuracy=letter_accuracy/num_batches\n",
        "        if self.is_train:\n",
        "            print(\"Train Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
        "            print(\"Train Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
        "        else:\n",
        "            print(\"Validation Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
        "            print(\"Validation Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
        "            \n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.show_accuracy_metrics(self.acc_batches)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vg-5BKWN9zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12ecc6c-950d-428b-eda3-55d28e7e6dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "early_stop=EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
        "model_chk_pt=ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False,save_weights_only=True,verbose=0, mode='auto', period=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4UC0x5LN9zt"
      },
      "source": [
        "**Labels Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJIBQOdeN9zt"
      },
      "outputs": [],
      "source": [
        "#Loading Train Data Labels\n",
        "Train_labels=[str(x) for x in train_data['Labels'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O1Oo1SMN9zt"
      },
      "outputs": [],
      "source": [
        "train_paths=[str(x) for x in train_data['ImageName'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd8pzRdaN9zt"
      },
      "outputs": [],
      "source": [
        "train_nan_cnt=0\n",
        "train_nan_replaced=False\n",
        "for i in range(len(Train_labels)):\n",
        "    if Train_labels[i]=='nan':\n",
        "        Train_labels[i]='NULL'\n",
        "        train_nan_replaced=True\n",
        "        train_nan_cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzEUV9udN9zt",
        "outputId": "edd19c3d-2047-419d-9b91-80ab33ba74ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Was there any NULL values written as Nan in Train Data: False\n",
            "Train Nan count:  0\n"
          ]
        }
      ],
      "source": [
        "print('Was there any NULL values written as Nan in Train Data:',train_nan_replaced)\n",
        "print('Train Nan count: ',train_nan_cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mHKKBopN9zt"
      },
      "outputs": [],
      "source": [
        "#Loading Validation Data Labels\n",
        "cv_labels=[str(x) for x in val_data['Labels'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5c8nGRDN9zu"
      },
      "outputs": [],
      "source": [
        "val_path=[str(x) for x in val_data['ImageName'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nutdcUo6N9zu"
      },
      "outputs": [],
      "source": [
        "val_nan_cnt=0\n",
        "val_nan_replaced=False\n",
        "for i in range(len(cv_labels)):\n",
        "    if cv_labels[i]=='nan':\n",
        "        cv_labels[i]='NULL'\n",
        "        val_nan_replaced=True\n",
        "        val_nan_cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MBJRRZNN9zu",
        "outputId": "ce831d9f-8307-4188-8a99-8d3c56363f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Was there any NULL values written as Nan : False\n",
            "Validation Nan count:  0\n"
          ]
        }
      ],
      "source": [
        "print('Was there any NULL values written as Nan :',val_nan_replaced)\n",
        "print('Validation Nan count: ',val_nan_cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik3VnmVBN9zu"
      },
      "source": [
        "**Instatiating Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWelvlnvN9zu"
      },
      "outputs": [],
      "source": [
        "train_gene=DataGenerator(train_paths,img_w, img_h,batch_size,200000,Train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tpcflbCN9zu",
        "outputId": "dcea3fb4-78be-4480-a146-a7799eb13764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000  Image Loading start...\n",
            "Loaded Images:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-259d73447d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_gene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-689ff82d2b32>\u001b[0m in \u001b[0;36mbuild_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "train_gene.build_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbxculw8N9zu"
      },
      "outputs": [],
      "source": [
        "train_num_batches=int(train_gene.n / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzaDI57kN9zv"
      },
      "outputs": [],
      "source": [
        "viz_cb_train = VizCallback( test_func, train_gene.next_batch(),True,train_num_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eJcPglrN9zv"
      },
      "outputs": [],
      "source": [
        "val_gen=DataGenerator(val_path,img_w, img_h,batch_size,12000,cv_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPjH8d22N9zv",
        "outputId": "e689bdf4-3036-4ee8-d208-07e8616688ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000  Image Loading start...\n",
            "Loaded Images:  0\n",
            "Number of Texts matches with Total Number of Images : False\n",
            "12000  Image Loading finish...\n"
          ]
        }
      ],
      "source": [
        "val_gen.build_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5M15guEN9zv"
      },
      "outputs": [],
      "source": [
        "val_num_batches=int(val_gen.n / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2jnTxJUN9zv"
      },
      "outputs": [],
      "source": [
        "viz_cb_val = VizCallback( test_func, val_gen.next_batch(),False,val_num_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOe4JK4mN9zv"
      },
      "source": [
        "**Defining Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw7pcUOHN9zv"
      },
      "outputs": [],
      "source": [
        "adam=optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1gNe9b0N9z1"
      },
      "outputs": [],
      "source": [
        "img_text_recog.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "logdir = os.path.join(\"logs_127\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "RlX61Ky9AtnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-uwuVUN9z1"
      },
      "source": [
        "### 12.1.1. Train Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_en_6IJ2N9z1",
        "outputId": "b86920b2-1726-467b-8a67-35cd03178a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-35a69dfa3988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m img_text_recog.fit_generator(generator=train_gene.next_batch(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gene\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mviz_cb_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mviz_cb_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_gene\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_chk_pt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2260\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/ctc/CTCLoss' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-46-35a69dfa3988>\", line 1, in <module>\n      img_text_recog.fit_generator(generator=train_gene.next_batch(),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2260, in fit_generator\n      return self.fit(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/core/lambda_layer.py\", line 196, in call\n      result = self.function(inputs, **kwargs)\n    File \"<ipython-input-16-6d8d7790134f>\", line 5, in ctc_loss_function\n      return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 6624, in ctc_batch_cost\n      tf.compat.v1.nn.ctc_loss(\nNode: 'model/ctc/CTCLoss'\nAll labels must be nonnegative integers, batch: 0 labels: -1,-1\n\t [[{{node model/ctc/CTCLoss}}]] [Op:__inference_train_function_13335]"
          ]
        }
      ],
      "source": [
        "img_text_recog.fit_generator(generator=train_gene.next_batch(),\n",
        "                    steps_per_epoch=int(train_gene.n / batch_size),\n",
        "                    epochs=20,\n",
        "                    callbacks=[viz_cb_train,viz_cb_val,train_gene,val_gen,tensorboard_callback,early_stop,model_chk_pt],\n",
        "                    validation_data=val_gen.next_batch(),\n",
        "                    validation_steps=int(val_gen.n / batch_size))\n",
        "#callbacks=[viz_cb_train,viz_cb_val,train_gene,val_gen,tensorboard_callback,early_stop,model_chk_pt],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvHoqMjON9z1"
      },
      "outputs": [],
      "source": [
        "img_text_recog.save('Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKTyDZrgN9z1"
      },
      "outputs": [],
      "source": [
        "img_text_recog.save('Img_recog_LSTM_Adam_model_run_3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aj9PPuuN9z5"
      },
      "source": [
        "## 13. Model Output Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9AERKZhN9z5"
      },
      "source": [
        "### 13.1. Best Path Decoding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n05akrMeN9z5"
      },
      "outputs": [],
      "source": [
        "def decode_label(out):\n",
        "    \"\"\"\n",
        "    Takes the predicted ouput matrix from the Model and returns the output text for the image\n",
        "    \"\"\"\n",
        "    out_best = list(np.argmax(out[0,2:], axis=1))\n",
        "\n",
        "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
        "\n",
        "    outstr=words_from_labels(out_best)\n",
        "    return outstr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdRS6AbFN9z5"
      },
      "source": [
        "### 13.2. Test Output Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDZ26WAYN9z5"
      },
      "outputs": [],
      "source": [
        "def test_data_output_Prediction(model,test_img_names,test_labels):\n",
        "    start=datetime.now()\n",
        "    accuracy=0\n",
        "    letter_acc=0\n",
        "    letter_cnt=0\n",
        "    count=0\n",
        "    letter_mis_match=[]\n",
        "    for i in range(len(test_labels)):\n",
        "        test_img=cv2.imread(test_img_names[i])\n",
        "        test_img_resized=cv2.resize(test_img,(170,32))\n",
        "        test_image=test_img_resized[:,:,1]\n",
        "        test_image=test_image.T\n",
        "        test_image=np.expand_dims(test_image,axis=-1)\n",
        "        test_image=np.expand_dims(test_image, axis=0)\n",
        "        test_image=test_image/255\n",
        "        model_output=model.predict(test_image)\n",
        "        predicted_output=decode_label(model_output)\n",
        "        actual_output=test_labels[i]\n",
        "        count+=1\n",
        "        mis_match=0\n",
        "        for j in range(min(len(predicted_output),len(actual_output))):\n",
        "            if predicted_output[j]==actual_output[j]:\n",
        "                letter_acc+=1\n",
        "            else:\n",
        "                mis_match+=1\n",
        "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
        "        letter_mis_match.append(mis_match)\n",
        "        if actual_output==predicted_output:\n",
        "            accuracy+=1\n",
        "        if (count%1000)==0:\n",
        "            print(\"Processed \",count,\" Images\")\n",
        "    print(\"Time Taken for Processing: \",datetime.now()-start)\n",
        "    return accuracy,letter_acc,letter_cnt,letter_mis_match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhvSMmZUN9z6"
      },
      "source": [
        "### 13.3. Model 1 Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmC9yLyaN9z6"
      },
      "outputs": [],
      "source": [
        "model=Image_text_recogniser_model_1('predict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H07OG4mQN9z6"
      },
      "outputs": [],
      "source": [
        "model.load_weights('Final_LSTM_Model_Best_Weights/Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD90WZovN9z6"
      },
      "source": [
        "**Synth Text Validation Data Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZDXSBU6N9z6"
      },
      "outputs": [],
      "source": [
        "val_img_names=val_data['ImageName'].values\n",
        "val_labels=val_data['Labels'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpXHkl7jN9z7"
      },
      "outputs": [],
      "source": [
        "synth_val_accuracy,synth_val_letter_acc,synth_val_letter_cnt,synth_val_mis_match=test_data_output_Prediction(model,val_img_names,val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7fCyqrFN9z7"
      },
      "outputs": [],
      "source": [
        "print(\"Model Output Accuracy: \",(synth_val_accuracy/len(val_labels))*100, \" %\")\n",
        "print(\"Model Output Letter Accuracy: \",(synth_val_letter_acc/synth_val_letter_cnt)*100, \" %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mobSNh7rN9z7"
      },
      "outputs": [],
      "source": [
        "model_1_val_mis_match_dict=Counter(synth_val_mis_match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dcd2MMuN9z7"
      },
      "source": [
        "**Model 1 Validation Data Prediction Analysis upto 4 Character Mis-Matches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVlT3_pvN9z7"
      },
      "outputs": [],
      "source": [
        "mis_match_cnts_1=[]\n",
        "for i in range(5):\n",
        "    mis_match_cnts_1.append(model_1_val_mis_match_dict[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_hdFWlzN9z7"
      },
      "outputs": [],
      "source": [
        "def mis_match_character_analysis_plot(mis_match_counts,num_values):\n",
        "    \"\"\"\n",
        "    Takes mis-match counts upto 4 characters of the predicted output, total number of values and\n",
        "    plots the percentage of number of mis-match characters between predicted and actual labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,6))\n",
        "    indices=np.arange(len(mis_match_counts))\n",
        "    counts=np.array(mis_match_counts)\n",
        "    percent=(counts/num_values)*100\n",
        "    plt.bar(indices,percent)\n",
        "    plt.xlabel('Number of Mis-Match Characters',fontsize=10)\n",
        "    plt.ylabel('Percentages',fontsize=10)\n",
        "    plt.title('Percentages of Number of Mis-Match Characters',fontsize=12)\n",
        "    plt.xticks(indices,indices)\n",
        "    plt.show()\n",
        "    for i in range(len(indices)):\n",
        "        print(i,\" Mis-Match Characters Percentage: \",np.round(percent[i],2),\" %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtOG4ViN9z7"
      },
      "source": [
        "**Synth Text Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6KZvbuaN9z8"
      },
      "outputs": [],
      "source": [
        "test_img_names=test_data['ImageName'].values\n",
        "test_labels=test_data['Labels'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqPoBgTFN9z8"
      },
      "outputs": [],
      "source": [
        "synth_test_accuracy,synth_test_letter_acc,synth_test_letter_cnt,synth_test_mis_match=test_data_output_Prediction(model,test_img_names,test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39T5nz7hN9z8",
        "outputId": "980f384d-fd29-47ca-af4b-955454eb7f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output Accuracy:  87.08666666666667  %\n",
            "Model Output Letter Accuracy:  94.48166697375254  %\n"
          ]
        }
      ],
      "source": [
        "print(\"Model Output Accuracy: \",(synth_test_accuracy/len(test_labels))*100, \" %\")\n",
        "print(\"Model Output Letter Accuracy: \",(synth_test_letter_acc/synth_test_letter_cnt)*100, \" %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PEzTHb9N9z8"
      },
      "outputs": [],
      "source": [
        "model_1_test_mis_match_dict=Counter(synth_test_mis_match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9B3aKEfN9z8"
      },
      "source": [
        "**Model 1 Test Data Prediction Analysis upto 4 Character Mis-Matches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDhU9f4FN9z8"
      },
      "outputs": [],
      "source": [
        "mis_match_cnts_2=[]\n",
        "for i in range(5):\n",
        "    mis_match_cnts_2.append(model_1_test_mis_match_dict[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRhDVatZN9z8"
      },
      "outputs": [],
      "source": [
        "mis_match_character_analysis_plot(mis_match_cnts_2,15000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnCweo8hN9z_"
      },
      "source": [
        "## 14. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWdho7iN9z_"
      },
      "source": [
        "1. **Model 1:** Model 1 Architecture consists of Convulution Layers, LSTM Units for RNN and uses Adam Optimizer\n",
        "2. **Val:** Denotes Synth Text Validation Data containing 12000 Images\n",
        "3. **Test:** Denotes Synth Text Test Data containing 15000 Images\n",
        "4. **0 MM(%):** Denotes % of points out of total points which have 0 character mis-match between Predicted and Actual Labels\n",
        "5. **1 MM(%):** Denotes % of points out of total points which have 1 character mis-match between Predicted and Actual Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp_58N4mN9z_",
        "outputId": "1bd762a1-ce46-40ef-d9a4-3b2f881bd648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------+----------+-----------------+---------+----------+\n",
            "|  Model  | Data | Accuracy | Letter Accuracy | 0 MM(%) | 1 MM (%) |\n",
            "+---------+------+----------+-----------------+---------+----------+\n",
            "| Model 1 | Val  |  86.47   |      94.01      |  86.58  |   5.78   |\n",
            "| Model 1 | Test |  87.98   |      94.48      |  87.19  |   5.85   |\n",
            "| Model 2 | Val  |  81.86   |       92.1      |  82.06  |   7.32   |\n",
            "| Model 2 | Test |  82.43   |      92.63      |  82.73  |   7.45   |\n",
            "+---------+------+----------+-----------------+---------+----------+\n"
          ]
        }
      ],
      "source": [
        "pt = PrettyTable()\n",
        "\n",
        "pt.field_names = [\"Model\", \"Data\", \"Accuracy\",\"Letter Accuracy\", \"0 MM(%)\", \"1 MM (%)\"]\n",
        "\n",
        "pt.add_row([\"Model 1\", \"Val\", 86.47, 94.01, 86.58, 5.78])\n",
        "\n",
        "pt.add_row([\"Model 1\", \"Test\", 87.98, 94.48, 87.19, 5.85])\n",
        "\n",
        "print(pt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}